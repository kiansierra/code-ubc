warmup_ratio: 0.1 
learning_rate: ${lr}
optim: adamw_torch
per_device_train_batch_size: ${train_batch_size}
per_device_eval_batch_size: ${eval_batch_size}
num_train_epochs: ${num_epochs}
report_to: wandb
output_dir:  ${output_dir}
overwrite_output_dir: True
# fp16: True
dataloader_num_workers: ${num_workers}
bf16: True
gradient_accumulation_steps: 8
logging_steps: 200
evaluation_strategy: 'steps'
eval_steps: 500
save_strategy: "steps"
save_steps: 500
load_best_model_at_end: True
metric_for_best_model: 'map@3'
greater_is_better: True
lr_scheduler_type: 'cosine'
weight_decay: 0.01
save_total_limit: 2
seed: ${seed}
remove_unused_columns: False